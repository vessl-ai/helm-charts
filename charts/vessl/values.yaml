agent:
  accessToken: # required
  clusterName:
  providerType: on-premise # on-premise, aws, gcp, oci, azure
  nginxEnabled: true
  imagePullPolicy:
  apiServer: https://api.vessl.ai
  logLevel: info
  env: prod
  image: "quay.io/vessl-ai/cluster-agent:0.6.24"
  sentryDsn: https://0481c31171114c109ac911ac947f0518@o386227.ingest.sentry.io/5585090
  scope: cluster # cluster, namespace
  containerRuntime: containerd # containerd, docker, crio
  clusterServiceType: Ingress # Ingress, LoadBalancer, NodePort
  region: "" # on-premise, <CLOUD_REGION>
  gpuPools: [] # this property is only used for the oci providerType
#    - name: "a10-test"
#      cpuLimit: 1
#      memoryLimit: "1Gi"
#      gpuLimit: 1
#      priority: 1
#      tolerations:
#        - key: "nvidia.com/gpu.present"
#          operator: "Exists"
#          effect: "NoSchedule"

  nodeSelector: {}
  tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Equal"
      value: ""
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "v1.k8s.vessl.ai/dedicated"
      operator: "Exists"
      effect: "NoSchedule"
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          preference:
            matchExpressions:
              - key: "node-role.kubernetes.io/master"
                operator: "In"
                values: [""]
        - weight: 1
          preference:
            matchExpressions:
              - key: "node-role.kubernetes.io/control-plane"
                operator: "Exists"
        - weight: 2
          preference:
            matchExpressions:
              - key: "v1.k8s.vessl.ai/dedicated"
                operator: "In"
                values: ["manager"]

# https://github.com/NVIDIA/k8s-device-plugin/tree/main/deployments/helm/nvidia-device-plugin
nvidia-device-plugin:
  enabled: true
  deviceListStrategy: volume-mounts
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              # On discrete-GPU based systems NFD adds the following label where 10de is te NVIDIA PCI vendor ID
              - key: "feature.node.kubernetes.io/pci-10de.present"
                operator: "In"
                values:
                  - "true"
          - matchExpressions:
              # On some Tegra-based systems NFD detects the CPU vendor ID as NVIDIA
              - key: "feature.node.kubernetes.io/cpu-model.vendor_id"
                operator: "In"
                values:
                  - "NVIDIA"
          - matchExpressions:
              # We allow a GFD deployment to be forced by setting the following label to "true"
              - key: "nvidia.com/gpu.present"
                operator: In
                values:
                  - "true"

# https://github.com/kubernetes-sigs/node-feature-discovery/tree/master/deployment/helm/node-feature-discovery
nfd:
  enabled: true
  master:
    tolerations:
      - operator: "Exists"
        effect: "NoSchedule"
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
                - key: "node-role.kubernetes.io/master"
                  operator: "In"
                  values: [""]
          - weight: 1
            preference:
              matchExpressions:
                - key: "node-role.kubernetes.io/control-plane"
                  operator: "Exists"
          - weight: 2
            preference:
              matchExpressions:
                - key: "v1.k8s.vessl.ai/dedicated"
                  operator: "In"
                  values: ["manager"]
    config:
      extraLabelNs: ["nvidia.com"]
  worker:
    tolerations:
      - operator: "Exists"
        effect: "NoSchedule"
    config:
      sources:
        pci:
          deviceClassWhitelist:
            - "02"
            - "0200"
            - "0207"
            - "0300"
            - "0302"
          deviceLabelFields:
            - "vendor"

# https://github.com/NVIDIA/gpu-feature-discovery/tree/main/deployments/helm/gpu-feature-discovery
gfd:
  enabled: true
  tolerations:
    - operator: "Exists"
      effect: "NoSchedule"
  nfd:
    enabled: false

# https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics
kube-state-metrics:
  enabled: true
  service:
    annotations:
      v1.k8s.vessl.ai/managed: "true"
      v1.k8s.vessl.ai/type: kube-state-metrics
  metricLabelsAllowlist:
  # This option makes `kube_pod_labels`, `kube_deployment_labels`, etc. contain all labels
  # (including those like `label_v1_k8s_vessl_ai_partition="xxxx"`) of the Kubernetes object.
  # This is used when aggregating values by Kubernetes resources' `*_vessl_ai_*` labels
  # to provide metrics and graphs.
  # The list should be extended when a new kind of resource requires label-joining.
    - deployments=[*]
    - jobs=[*]
    - nodes=[*]
    - pods=[*]
    - replicasets=[*]
    - statefulsets=[*]
  tolerations:
    - key: "node-role.kubernetes.io/master"
      operator: "Equal"
      value: ""
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "v1.k8s.vessl.ai/dedicated"
      operator: "Exists"
      effect: "NoSchedule"
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          preference:
            matchExpressions:
              - key: "node-role.kubernetes.io/master"
                operator: "In"
                values: [""]
        - weight: 1
          preference:
            matchExpressions:
              - key: "node-role.kubernetes.io/control-plane"
                operator: "Exists"
        - weight: 2
          preference:
            matchExpressions:
              - key: "v1.k8s.vessl.ai/dedicated"
                operator: "In"
                values: ["manager"]

# https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter
node-exporter:
  enabled: true
  service:
    annotations:
      prometheus.io/scrape: "true"
      v1.k8s.vessl.ai/managed: "true"
      v1.k8s.vessl.ai/type: node-exporter

# https://github.com/NVIDIA/dcgm-exporter/tree/main/deployment
dcgm-exporter:
  enabled: true
  service:
    annotations:
      prometheus.io/scrape: "true"
      v1.k8s.vessl.ai/managed: "true"
      v1.k8s.vessl.ai/type: dcgm-exporter
  serviceMonitor:
    enabled: false
  kubeletPath: "/var/lib/kubelet/pod-resources"
  tolerations:
    - operator: "Exists"
      effect: "NoSchedule"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              # On discrete-GPU based systems NFD adds the following label where 10de is te NVIDIA PCI vendor ID
              - key: "feature.node.kubernetes.io/pci-10de.present"
                operator: "In"
                values:
                  - "true"
          - matchExpressions:
              # On some Tegra-based systems NFD detects the CPU vendor ID as NVIDIA
              - key: "feature.node.kubernetes.io/cpu-model.vendor_id"
                operator: "In"
                values:
                  - "NVIDIA"
          - matchExpressions:
              # We allow a GFD deployment to be forced by setting the following label to "true"
              - key: "nvidia.com/gpu.present"
                operator: In
                values:
                  - "true"

# https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus
prometheus-remote-write:
  enabled: true
  server:
    configMapOverrideName: vessl-prometheus-scrape-config
    persistentVolume:
      enabled: false
    # https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write
    remoteWrite:
      - name: "vessl-remote-write"
        url: "https://remote-write-gateway.vessl.ai/remote-write"
        authorization:
          type: "Token"
          credentials_file: "/etc/secrets/token"
        write_relabel_configs:
          - action: labeldrop
            regex: feature_node_kubernetes_io_(.+)
          - action: labeldrop
            regex: label_feature_node_kubernetes_io_(.+)
          - action: labeldrop
            regex: minikube_(.+)
          - action: labeldrop
            regex: cloud_google_com_(.+)
    additionalScrapeJobs: []
    extraSecretMounts:
      - name: "access-token"
        secretName: "vessl-agent"
        mountPath: "/etc/secrets"
        readOnly: true
    tolerations:
      - key: "node-role.kubernetes.io/master"
        operator: "Equal"
        value: ""
        effect: "NoSchedule"
      - key: "node-role.kubernetes.io/control-plane"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "v1.k8s.vessl.ai/dedicated"
        operator: "Exists"
        effect: "NoSchedule"
    affinity:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
                - key: "node-role.kubernetes.io/master"
                  operator: "In"
                  values: [""]
          - weight: 1
            preference:
              matchExpressions:
                - key: "node-role.kubernetes.io/control-plane"
                  operator: "Exists"
          - weight: 2
            preference:
              matchExpressions:
                - key: "v1.k8s.vessl.ai/dedicated"
                  operator: "In"
                  values: ["manager"]
  alertmanager:
    enabled: false
  kube-state-metrics:
    enabled: false
  prometheus-node-exporter:
    enabled: false
  prometheus-pushgateway:
    enabled: false
  relabelConfigs:
  - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]
    separator: ;
    regex: Node;(.*)
    target_label: node
    replacement: ${1}
    action: replace
  - source_labels: [__meta_kubernetes_endpoint_address_target_kind, __meta_kubernetes_endpoint_address_target_name]
    separator: ;
    regex: Pod;(.*)
    target_label: pod
    replacement: ${1}
    action: replace
  - source_labels: [__meta_kubernetes_namespace]
    separator: ;
    regex: (.*)
    target_label: namespace
    replacement: $1
    action: replace
  - source_labels: [__meta_kubernetes_service_name]
    separator: ;
    regex: (.*)
    target_label: service
    replacement: $1
    action: replace
  - source_labels: [__meta_kubernetes_pod_name]
    separator: ;
    regex: (.*)
    target_label: pod
    replacement: $1
    action: replace
  - source_labels: [__meta_kubernetes_pod_container_name]
    separator: ;
    regex: (.*)
    target_label: container
    replacement: $1
    action: replace
  - source_labels: [__meta_kubernetes_service_name]
    separator: ;
    regex: (.*)
    target_label: job
    replacement: ${1}
    action: replace
  - source_labels: ["__meta_kubernetes_pod_label_v1_k8s_vessl_ai_partition"]
    target_label: "vessl_partition"
  - source_labels: ["__meta_kubernetes_pod_label_v1_k8s_vessl_ai_type"]
    target_label: "vessl_type"
  - separator: ;
    regex: (.*)
    target_label: endpoint
    replacement: metrics
    action: replace

# https://github.com/rancher/local-path-provisioner/tree/master/deploy/chart/local-path-provisioner
local-path-provisioner:
  enabled: true
  storageClass:
    name: "vessl-local-path"
    provisionerName: "rancher.io/local-path"
  helperImage:
    repository: "quay.io/vessl-ai/busybox"
    tag: "21.10"
  tolerations:
    - key: "node-role.kubernetes.io/master"
      value: ""
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "v1.k8s.vessl.ai/dedicated"
      operator: "Exists"
      effect: "NoSchedule"
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          preference:
            matchExpressions:
              - key: "node-role.kubernetes.io/master"
                operator: "In"
                values: [""]
        - weight: 1
          preference:
            matchExpressions:
              - key: "node-role.kubernetes.io/control-plane"
                operator: "Exists"
        - weight: 2
          preference:
            matchExpressions:
              - key: "v1.k8s.vessl.ai/dedicated"
                operator: "In"
                values: ["manager"]
  configmap:
    create: true

vessl-nginx:
  enabled: true
  controller:
    ingressClass:
      name: "vessl-nginx"
    service:
      # This will create nodePort service on cluster biding port 80 and 443.
      # Set this to false if cluster already has other ingress controller, such as ingress-nginx
      create: false
    # enableSnippets must be true to run VESSL service in nodeport mode
    enableSnippets: true


lpp-advanced-config:
  enabled: false
  xfs_quota:
    enabled: true
    size: 10g

# https://github.com/longhorn/longhorn/tree/master/chart
longhorn:
  enabled: false
  # https://longhorn.io/docs/1.5.1/advanced-resources/deploy/customizing-default-settings/#using-helm
  persistence:
    defaultClassReplicaCount: 1
    defaultDataLocality: strict-local
  defaultSettings:
    defaultReplicaCount: 1
    defaultDataLocality: strict-local
    taintToleration: node-role.kubernetes.io/master=:NoSchedule; node-role.kubernetes.io/control-plane=:NoSchedule; v1.k8s.vessl.ai/dedicated:NoSchedule

# https://github.com/vessl-ai/helm-charts/blob/main/charts/vessl/charts/harbor/values.yaml
harbor:
  enabled: false

# https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-adapter
prometheus-adapter:
  enabled: false

  tolerations:
    - key: "node-role.kubernetes.io/master"
      value: ""
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "v1.k8s.vessl.ai/dedicated"
      operator: "Exists"
      effect: "NoSchedule"

  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 1
          preference:
            matchExpressions:
              - key: "node-role.kubernetes.io/master"
                operator: "In"
                values: [""]
        - weight: 1
          preference:
            matchExpressions:
              - key: "node-role.kubernetes.io/control-plane"
                operator: "Exists"
        - weight: 2
          preference:
            matchExpressions:
              - key: "v1.k8s.vessl.ai/dedicated"
                operator: "In"
                values: ["manager"]

  prometheus:
    url: http://vessl-prometheus-remote-write-server.vessl.svc

  rules:
    default: false

    custom:
      - seriesQuery: 'DCGM_FI_DEV_GPU_UTIL{pod!=""}'
        resources:
          overrides:
            namespace: {resource: "namespace"}
            pod: {resource: "pod"}
        name:
          as: "gpu_util"
        metricsQuery: sum(<<.Series>>{<<.LabelMatchers>>}) by (<<.GroupBy>>)

image-prepull:
  enabled: false
